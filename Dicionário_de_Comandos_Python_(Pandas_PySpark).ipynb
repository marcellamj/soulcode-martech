{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgkDdFX32cBZ7lPKoaBeNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcellamj/soulcode-martech/blob/main/Dicion%C3%A1rio_de_Comandos_Python_(Pandas_PySpark).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dicionário de Comandos**"
      ],
      "metadata": {
        "id": "yUoP6bwUQOS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pandas**"
      ],
      "metadata": {
        "id": "DL5R31p3QJ6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importações**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "!pip install gcsfs\n",
        "!pip install pandera\n",
        "```\n",
        "\n",
        "```\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandera as pa\n",
        "from google.cloud import storage\n",
        "```\n",
        "\n",
        "```\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "```"
      ],
      "metadata": {
        "id": "xZ4-DjOmEwYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EXTRAÇÃO**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'caminho_arquivo'\n",
        "df = pd.read_csv(path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FqLJUZ-dhXen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos**\n",
        "\n",
        "* **df** - mostra o início e fim da tabela\n",
        "* **df.samples(5)** - mostra linhas aleatórias da tabela (número de linhas)\n",
        "* **df.shape** - número de linhas e colunas da tabela\n",
        "* **df.dtypes** - tipo de cada atributo\n",
        "* **df.count()** - contagem de valores válidos (não nulos) em cada atributo\n",
        "* **df.info()** - informações gerais dos atributos: tipo e quantidade de cada um, contagem de valores não nulos\n",
        "* **dfback1 = df.copy()** - backup1 local dos dados\n",
        "* **df = dfback1** - trazer um backup salvo"
      ],
      "metadata": {
        "id": "-M0WEldHE5k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRANSFORMAÇÃO**"
      ],
      "metadata": {
        "id": "H23nL3swE5fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos**\n",
        "\n",
        "* **.all()** - verifica se colunas são iguais, linhas a linha - usado em conjunto com o IF/ELSE\n",
        "\n",
        "* **colunas_verificar = ['codigo_ocorrencia1', 'codigo_ocorrencia2', 'codigo_ocorrencia3', 'codigo_ocorrencia4']** - criar uma variável lista para checagem, fórmulas\n",
        "\n",
        "* **df.coluna.is_unique** - verifica se dados na coluna são únicos, se não há valores repetidos - funciona bem para índices\n",
        "\n",
        "* **df.isna.sum()** - verifica dados nulos, ausentes\n",
        " * **df.isnull.sum()** - outra forma de fazer\n",
        " * **(df.isna.sum() / len(df)) * 100** - verificaçáo da porcentagem de nulos (soma nulos/len)\n",
        "\n",
        "* **df.drop(n_linha)** - exclui colunas\n",
        "  * ([ ' ' ],axis=1, inplace=True) - estrutura do drop entre parênteses\n",
        "\n",
        "* **df.head(5)** - exibe apenas 5 linhas do topo\n",
        "\n",
        "* **df.tail(5)** - exibe apenas 5 linhas do final ou quantas quiser, só trocar o número\n",
        "\n",
        "* **print(pd.unique(df['coluna']))** - exibe os valores únicos de uma coluna específica\n",
        "\n",
        "* **print(sorted(pd.unique(df['coluna'])))** - exibe os valores únicos de uma coluna específica em ordem crescente"
      ],
      "metadata": {
        "id": "_TuPOUX6FElT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Trocar valores\n",
        "* **pd.NA**\n",
        " * **df.replace(['*'], pd.NA, inplace=True)** - troca um valor estranho em dado não numérico por um valor ausente, utilizando o método replace()\n",
        "\n",
        "* **np.NaN**\n",
        " * **df.replace(['*'], np.NaN, inplace=True)** - troca um valor estranho em dado numérico por um valor ausente, utilizando o método replace()\n",
        "\n",
        "* **df = df.dropna()** - remove linhas com valores NaN\n",
        "\n",
        "* **df.rename(columns={'coluna':'nome_novo'}, inplace=True)** - renomear a(s) colunas de um dataframe"
      ],
      "metadata": {
        "id": "UfdQHEnkFH76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Trocar tipos de dados\n",
        "* **.astype(tipo)** - troca os valores da coluna por outro tipo de dado\n",
        "  * **ex.: df['coluna']=df['coluna'].astype(float)\n",
        "  * **ex.: df['coluna']=df['coluna'].astype('datetime[ns]')\n",
        "\n",
        "* **df['ano'] = df.data.dt.year** - Ano - cria uma nova coluna só com o valor do \"ano\" a partir dos valores da coluna \"data\" (depois de ter tipado a coluna data como datetime)\n",
        "\n",
        "* **df['mês'] = df.data.dt.month** - Mês\n",
        "\n",
        "* **df['dia'] = df.data.dt.day** - Dia\n",
        "\n",
        "* **.split(':')** - separar uma coluna definindo o delimitador\n",
        "  * **ex.: df['hora'] = df['hora'].str.split(':').str[0] - salva apenas a parte da \"hora\" antes do delimitador\n",
        "\n",
        "* **df[['coluna1', 'coluna2']] = df[['coluna1', 'coluna2']].round(2)** - arredondar valores de várias colunas"
      ],
      "metadata": {
        "id": "qzTim_SkE-XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Modificações no df\n",
        "* **df_pd['sexo'] = ['M','F','M','F','M','F']** - adicionar linha ao df"
      ],
      "metadata": {
        "id": "GXzsgqQyih6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Merge\n",
        "\n",
        "* **uniao=pd.concat([df_pd1,df_pd2], ignore_index=True)** - concatenar dois DataFrames Pandas"
      ],
      "metadata": {
        "id": "MXBM54zUg5gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exemplo de .all()**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "if (df['codigo_ocorrencia'] == df['codigo_ocorrencia1']).all():\n",
        "    print(\"As colunas são iguais.\")\n",
        "else:\n",
        "    print(\"As colunas são diferentes.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3s8jFU35GO9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validação Schema**\n",
        "* **pa.schema** - validação dos tipos de dados de todas as colunas do DataFrame\n",
        "\n",
        "- Definir o esquema de validação\n",
        "- Nome da coluna e tipo\n",
        "- Validar Dataframe\n",
        "\n",
        "```\n",
        "schema = pa.DataFrameSchema({'coluna': pa.Column(pa.String),\n",
        "                            'classificacao': pa.Column(pa.String),\n",
        "                            'cidade': pa.Column(pa.String),\n",
        "                            'uf': pa.Column(pa.String),\n",
        "                            'data': pa.Column(pa.DateTime),\n",
        "                            'n_aeronaves': pa.Column(pa.Float)\n",
        "                            })\n",
        "\n",
        "schema.validate(df)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sZfNSJf-TDRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CARREGAMENTO**"
      ],
      "metadata": {
        "id": "HoatlxMTE5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive - Utilizar se for o rotulo padrão\n",
        "\n",
        "df.to_csv('caminho_arquivo_tratado',index=False)"
      ],
      "metadata": {
        "id": "sXaPZEx4UDl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCP - Salvar no bucket\n",
        "\n",
        "df.to_csv('caminho_arquivo_tratado', index=False)"
      ],
      "metadata": {
        "id": "Xa6Msv2uwONC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANÁLISES**"
      ],
      "metadata": {
        "id": "zujluuagUO8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos Localização**\n",
        "\n",
        "Localizar\n",
        "* **df.loc[5]** - dados de uma linha específica\n",
        "\n",
        "* **df.iloc[0]** - dados de uma linha com índice\n",
        "\n",
        "* **df.iloc[0][0]** - dados de uma linha e coluna\n",
        "\n",
        "* **df_pd[['nome','peso']].iloc[1:4][0:2]** - intervalo de colunas e linhas\n",
        "\n",
        "* **df.loc[1000:1002]** - dados de um conjunto de linhas\n",
        "\n",
        "* **df.loc[1000:1002], ['cidade', 'uf']]** - dados de um conjunto de linhas e colunas\n",
        "\n",
        "***\n",
        "Alterações\n",
        "* **df.loc[0,'coluna'] = 'CAMPO GRANDE'** - troca um valor em uma linha e coluna específica\n",
        "\n",
        "* **df.loc[:,'coluna'] = 'CAMPO GRANDE'** - troca um valor da coluna inteira - menos utilizado\n",
        "\n",
        "* **df.loc[df.coluna == 'VALOR', ['coluna']] = 'VALOR_NOVO'** - pesquisa um valor em uma coluna e substitui todos os que forem localizados\n"
      ],
      "metadata": {
        "id": "sfugxXnIUDt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos Filtro**\n",
        "\n",
        "* **filtro = df.coluna == 'VALOR'** - seleciona em uma coluna apenas um valor definido\n",
        "  * Podemos criar um novo dataframe para salvar o filtro:\n",
        "```\n",
        "df_filtro = df.loc[filtro]\n",
        "df_filtro\n",
        "```\n",
        "\n",
        "* Guarda posições de valores ausentes\n",
        "```\n",
        "filtro_nulo = df.coluna.isna()\n",
        "```\n",
        "* mostra as linhas com valor ausente\n",
        "```\n",
        "df.loc[filtro_nulo].head()\n",
        "```\n",
        "*  Últimas 3 letras sendo RIO\n",
        "```\n",
        "filtro_letras = df.cidade.str[-3:] == 'RIO'\n",
        "```\n",
        "* Contendo letras/string em qualquer lugar da palavra\n",
        "```\n",
        "filtro_interno = df.cidade.str.contains('BO|MA')\n",
        "```\n",
        "* Localiza o ano 2019 na coluna data\n",
        "```\n",
        "df.loc[(df['data']).dt.year == 2019]\n",
        "```\n",
        "* Período de tempo\n",
        "```\n",
        "ft_quinz = (df.data.dt.day > 0) & (df.data.dt.day < 16) -\n",
        "```"
      ],
      "metadata": {
        "id": "8ptoQQJ5W5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtro com mais detalhes ao mesmo tempo:\n",
        "\n",
        "ft_cidade = df.cidade == 'SÃO PAULO'\n",
        "ft_pista = df.saida_pista == 'SIM'\n",
        "ft_2021 = df.data.dt.year == 2021\n",
        "ou\n",
        "ft_2021 = df.ano == 2021\n",
        "ft_uf = df.uf == 'SP'\n",
        "\n",
        "# Localiza e mostra as linhas que tiverem os valores definidos\n",
        "df.loc[ft_cidade & ft_pista & ft_2021 & ft_uf]"
      ],
      "metadata": {
        "id": "FuKDMSE0Yxb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos Groupby**\n",
        "\n",
        "\n",
        "*   **df.groupby(['coluna']).size()** - contagem da coluna\n",
        "*   **df.groupby(['coluna']).size().sort_values(ascending=False)** - contagem de valores da coluna do maior para o menor\n",
        "*  **df.groupby('coluna')['coluna que quero contar'].sum().sort_values(ascending=False)** - mostrar uma contagem com atributos específicos\n",
        "* ex.:\n",
        "  ```\n",
        "   df.groupby('cidade')['n_aeronaves'].sum().sort_values(ascending=False).head(10)\n",
        "  ```\n",
        "* Contagem considerando valores ausentes:\n",
        "\n",
        " ```\n",
        " df.groupby(['coluna'],dropna=False).size().sort_values(ascending=False).head(10)\n",
        " ```\n",
        "* Contagem do mais frequente: **moda**:\n",
        "```\n",
        " df.groupby(['coluna'],dropna=False).count().sort_values(ascending=False).first()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7XtiGezMa7vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# FILTRO + GROUPBY\n",
        "\n",
        "# Filtros\n",
        "filtro_incidente = df.classificacao =='INCIDENTE'\n",
        "filtro_sul = df.uf.isin(['RS','PR','SC'])                 # método isin() usado para selecionar as regiões.\n",
        "\n",
        "# Filtros juntos: incidentes e sul\n",
        "dfsul = df.loc[filtro_incidente & filtro_sul]             # incidentes dos Estados da Região Sul\n",
        "\n",
        "# Agrupamento\n",
        "dfsul.groupby(['uf']).size().sort_values(ascending=False) # Contagem\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8j1Z_HBOprHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos Medidas Descritivas**\n",
        "\n",
        "* medidas de count, mean, stddev, min, max\n",
        "  ```\n",
        "  df.describe()\n",
        "  ```\n",
        "* medidas de count, mean, stddev, min, 25, 50, 75, max\n",
        "  ```\n",
        "  df.summary()\n",
        "  ```\n",
        "* medidas de moda/multimoda\n",
        "  ```\n",
        "  mode([1,2])\n",
        "  ```\n",
        "  ```\n",
        "  multimode([1,2,2,3,3])\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "KeatutfAPGUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PySpark**"
      ],
      "metadata": {
        "id": "gtfwIpmxP2bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importações**\n",
        "\n",
        "* Instalar no computador do Google\n",
        "* Importar ferramentas do ambiente clusterizado\n",
        "* Criar sessão Spark em um cluster e deixar visualização das tabelas mais amigável"
      ],
      "metadata": {
        "id": "gkxlOWXKP9yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# caminho Apache, se não funcionar colocar nos arquivos do Colab e copiar caminho\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "```\n",
        "\n",
        "```\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "```\n",
        "\n",
        "```\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Para deixar a visualição das tabelas mais amigável\n",
        "spark\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GZVRj5EjpYOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EXTRAÇÃO**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'caminho_arquivo'\n",
        "df = pd.read_csv(path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xH4WTqiiQAVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar Dataframe PySpark"
      ],
      "metadata": {
        "id": "SchwJA9plK3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importando estrutura da tabela (Structfield)\n",
        "* Criação do Schema, esqueleto da tabela\n",
        "* Criação do objeto, nível Python, que contém as tuplas com os registros que farão parte da tabela\n",
        "* Criação do dataframe py spark, juntando a estrutura da tabela com o objeto que contém os registros\n",
        "\n",
        "- A função **StructType** é usada para definir a estrutura de uma tabela\n",
        "- A função **StructField** é usada para definir os atributos da tabela\n",
        "- (String, Integer -inteiro-, Double -float-)**Type** define o tipo da variável\n",
        "- True significa que aceita valor nulo"
      ],
      "metadata": {
        "id": "NYuInOUtiFKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DoubleType\n",
        "schema = StructType([ \\\n",
        "    StructField('coluna',StringType(),True), \\\n",
        "    StructField('coluna',IntegerType(),True), \\\n",
        "    StructField('coluna',DoubleType(),True), \\\n",
        "  ])\n",
        "dados = [('x',y,z.),\n",
        "    ('x',y,z.),\n",
        "    ('x',y,z.),\n",
        "    ('x',y,z.),\n",
        "  ]\n",
        "\n",
        "df_py = spark.createDataFrame(data=dados,schema=schema)\n",
        "df_py.printSchema()\n",
        "df_py.show()"
      ],
      "metadata": {
        "id": "Dfm5cGnTeIAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar Dataframe PySpark a partir de df Pandas\n",
        "\n",
        "```\n",
        "df_py = spark.createDataFrame(df_pd)\n",
        "df_py.printSchema()\n",
        "df_py.show()\n",
        "```"
      ],
      "metadata": {
        "id": "HcRoNrVJliOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos**\n",
        "\n",
        "* **df_py.printSchema()** - mostra tipo de dado das colunas\n",
        "\n",
        "* **df_py.describe()** - medidas descritivas de count, mean, stddev, min, max\n",
        "\n",
        "* **df_py.head(5)** - exibe apenas 5 linhas do topo\n",
        "\n",
        "* **df_py.tail(5)** - exibe apenas 5 linhas do final ou quantas quiser, só trocar o número\n",
        "\n",
        "* **df_py.collect()[0]** - seleciona 1ª linha do df\n",
        "\n",
        "* **df_py.collect()[0][0]** - seleciona dado específico de [linha][colun]\n",
        "\n",
        "* **df_ex.orderBy(\"coluna\",ascending=True/False)** - ordena df por coluna específica\n",
        "  * também pode usar o sort(\"coluna\")"
      ],
      "metadata": {
        "id": "8HF83lRSQB0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRANSFORMAÇÃO**"
      ],
      "metadata": {
        "id": "gwYL5RtVQGsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos**\n",
        "\n",
        "Alterações\n",
        "* **** - insere colunas\n",
        "* **df_py.drop(\"\")** - exclui colunas\n",
        "\n",
        "***\n",
        "Filter\n",
        "\n",
        "* **df_py.filter(df_py.nome == ' ')** - filtra por valor da coluna\n",
        " * **df_py.where(df_py.nome == ' ')** - outro jeito\n",
        "\n",
        "***\n",
        "Merge\n",
        "\n",
        "* **df_py2 = df_py.union(df_py1)** - unir as linhas de 2 dataframes com os mesmos atributos\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "srI6c6foQGss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renomear coluna\n",
        "* **df_py = df_py.withColumnRenamed(\"a\",\"b\").withColumnRenamed(\"x\",\"y\")** - renomear colunas"
      ],
      "metadata": {
        "id": "Tb5KHvbLQGst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validação Schema**\n",
        "* **pa.schema** - validação dos tipos de dados de todas as colunas do DataFrame\n",
        "\n",
        "- Definir o esquema de validação\n",
        "- Nome da coluna e tipo\n",
        "- Validar Dataframe\n",
        "\n",
        " ```\n",
        " schema = pa.DataFrameSchema({'coluna': pa.Column(pa.String),\n",
        "                            'classificacao': pa.Column(pa.String),\n",
        "                            'cidade': pa.Column(pa.String),\n",
        "                            'uf': pa.Column(pa.String),\n",
        "                            'data': pa.Column(pa.DateTime),\n",
        "                            'n_aeronaves': pa.Column(pa.Float)\n",
        "                            })\n",
        "\n",
        " schema.validate(df)\n",
        " ```\n",
        "\n"
      ],
      "metadata": {
        "id": "npufv5sRQGsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adicionar colunas ao df_py via mudança de schema\n",
        "\n",
        " ```\n",
        " dados = [('Douglas',45,1.85,70.,'M'),\n",
        "    ('Daniela',7,1.23,22.,'F'),\n",
        "    ('Pedro',65,1.75,87.,'M'),\n",
        "    ('Maria',64,1.67,64.,'F'),\n",
        "    ('Eduardo',37,1.82,96.,'M'),\n",
        "    ('Ester',37,1.73,68.,'F')\n",
        "  ]\n",
        "\n",
        " schema = StructType([ \\\n",
        "    StructField('nome',StringType(),True), \\\n",
        "    StructField('idade',IntegerType(),True), \\\n",
        "    StructField('altura',DoubleType(),True), \\\n",
        "    StructField('peso', DoubleType(), True), \\\n",
        "    StructField('sexo', StringType(), True), \\\n",
        "  ])\n",
        "\n",
        " df_py = spark.createDataFrame(data=dados,schema=schema)\n",
        " df_py.printSchema()\n",
        " df_py.show(truncate=False)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SWA2euUkio84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adicionar linha ao df_py\n",
        "\n",
        " ```\n",
        " nova_linha = spark.createDataFrame([('Tobias',18,1.67,65.,'M')])\n",
        " df_py = df_py.union(nova_linha)\n",
        " df_py.show()\n",
        " ```\n",
        "\n"
      ],
      "metadata": {
        "id": "A4ZPjEvxjV6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANÁLISES**"
      ],
      "metadata": {
        "id": "l988fbSYp7rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comandos Medidas Descritivas**\n",
        "\n",
        "* medidas de count, mean, stddev, min, max\n",
        "  ```\n",
        "  df.describe()\n",
        "  ```\n",
        "* medidas de count, mean, stddev, min, 25, 50, 75, max\n",
        "  ```\n",
        "  df.summary()\n",
        "  ```\n",
        "\n",
        "* Q1, Q2, Q3\n",
        " ```\n",
        " df_py.approxQuantile(['coluna1','coluna2','coluna3'],[0.25,0.5,0.75],0)\n",
        " ```\n",
        "\n",
        "* AT\n",
        "  ```\n",
        " df_py.approxQuantile(['coluna1','coluna2','coluna3'],[0.25,0.5,0.75],0)\n",
        " ```\n",
        "* AIQ\n",
        " ```\n",
        " df_py.approxQuantile(['coluna1','coluna2','coluna3'],[0.25,0.5,0.75],0)\n",
        " ```\n",
        "* LS\n",
        " ```\n",
        " df_py.approxQuantile(['coluna1','coluna2','coluna3'],[0.25,0.5,0.75],0)\n",
        " ```\n",
        "* LI\n",
        " ```\n",
        " df_py.approxQuantile(['coluna1','coluna2','coluna3'],[0.25,0.5,0.75],0)\n",
        " ```"
      ],
      "metadata": {
        "id": "eHCDeuwGp7ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automação - Listar colunas e calcular medidas\n",
        "(Diego, Aska, Samira, Gus)\n",
        "```\n",
        "from pyspark.sql.functions import col\n",
        "#Lista todas as Colunas do dataframe\n",
        "todas_colunas = [coluna for coluna, tipo in df_ex.dtypes ]\n",
        "#Lista todas as Colunas do dataframe\n",
        "colunas_numericas = [coluna for coluna, tipo in df_ex.dtypes if tipo in ['double', 'float', 'decimal', 'int', 'long']]\n",
        "# Calculando os quartis de todos os atributos quantitativos\n",
        "quartis = df_ex.approxQuantile(colunas_numericas, [0.25,0.5,0.75], 0)\n",
        "print(colunas_numericas,quartis)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CHCSG8VDGpr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataViz"
      ],
      "metadata": {
        "id": "gk0SjgHOQIUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualização Dinâmica de Dados**\n",
        "\n",
        "```\n",
        "!pip install pygwalker -q\n",
        "import pygwalker as pyg\n",
        "pyg.walk(df)\n",
        "```"
      ],
      "metadata": {
        "id": "gAbsHK0iQKfr"
      }
    }
  ]
}